{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90d8a16e-e083-498c-a301-426330554674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total GPU memory: 47.544312 GB\n",
      "available GPU memory 2.184448 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "from tqdm import tqdm \n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import logging\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "\n",
    "print(\"total GPU memory: %f GB\" % (torch.cuda.mem_get_info()[1] / 1024 ** 3))\n",
    "print(\"available GPU memory %f GB\" % (torch.cuda.mem_get_info()[0] / 1024 ** 3))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    logging.warning(\"No GPU available, using CPU will be super slow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2573b9-72b9-4267-91a5-2e9dcefb5d76",
   "metadata": {},
   "source": [
    "### Load dataset, model, and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f68e2ea-76b9-4059-b809-c7d643692e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"ag_news\"\n",
    "model_id = \"roberta-base\"\n",
    "save_dir = \"./ckpt/lora_roberta_agnews\"\n",
    "\n",
    "dataset = load_dataset(dataset_id)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset[\"test\"].shard(num_shards=2, index=0)\n",
    "val_dataset = dataset[\"test\"].shard(num_shards=2, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e008657e-0618-45ac-8c2f-83a6b33214fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b83a821-e392-4160-8555-4c500edc028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 4\n",
      "the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "num_labels = dataset['train'].features['label'].num_classes\n",
    "class_names = dataset[\"train\"].features[\"label\"].names\n",
    "\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9807fbe7-5dc4-4ba3-b65a-47de37aa56ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"World\",\n",
       "    \"1\": \"Sports\",\n",
       "    \"2\": \"Business\",\n",
       "    \"3\": \"Sci/Tech\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.33.2\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "config.update({\"id2label\": id2label})\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd56bdcf-b08e-4ca0-b600-e068fa0dbaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,363,208 || all params: 125,316,104 || trainable%: 1.0878154973601797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['query', 'value']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_model = RobertaForSequenceClassification.from_pretrained(model_id, config=config).to(device)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False, r=2, lora_alpha=2, lora_dropout=0.1, bias=\"all\",\n",
    ")\n",
    "roberta_model = get_peft_model(roberta_model, peft_config)\n",
    "roberta_model.print_trainable_parameters()\n",
    "peft_config.target_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e143481c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size = 475.50 MB\n",
      "Number of parameters = 124.65 M\n"
     ]
    }
   ],
   "source": [
    "# check param count and mem usage \n",
    "# too small for LoRA to save training time\n",
    "mem_params = sum([param.nelement()*param.element_size() for param in roberta_model.parameters()])\n",
    "mem_bufs = sum([buf.nelement()*buf.element_size() for buf in roberta_model.buffers()])\n",
    "mem = mem_params + mem_bufs # in bytes\n",
    "param_count = sum([param.nelement() for param in roberta_model.parameters()])\n",
    "print(f\"Model size = {mem/1024**2:.2f} MB\")\n",
    "print(f\"Number of parameters = {param_count/1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a2748-204b-4c34-9f92-5d6cc5ebf52f",
   "metadata": {},
   "source": [
    "### Evaluate performance before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b24d376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3800/3800 [00:46<00:00, 81.44it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.autocast(device):\n",
    "    roberta_predictions = []\n",
    "    for i in tqdm(range(len(test_dataset[\"text\"])), total=len(test_dataset[\"text\"])):\n",
    "        test_input = tokenizer(test_dataset[\"text\"][i], return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = roberta_model(**test_input).logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "        roberta_predictions.append(predicted_class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a50cb4f-dad0-49ce-b447-b1e1ab0baf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw roberta accuracy:  0.249\n"
     ]
    }
   ],
   "source": [
    "print(\"raw roberta accuracy: \", round(accuracy_score(test_dataset[\"label\"], roberta_predictions), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f853f4-175c-4078-96f8-49422be48740",
   "metadata": {},
   "source": [
    "### Fine-tune Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c491439-ae71-4dd8-a3e1-2deb76358563",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=save_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=180,\n",
    "    per_device_eval_batch_size=380,\n",
    "    learning_rate=0.005,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    warmup_steps=50,\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f6a7a73-0d16-43ab-9338-ee3449498b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89bdbcab-9766-4986-a450-638cef6dec83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='667' max='667' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [667/667 08:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.223473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.autocast(device, cache_enabled=False):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f048aec-4b59-4709-b686-3cf5a439816b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.223463237285614,\n",
       " 'eval_runtime': 12.8218,\n",
       " 'eval_samples_per_second': 296.37,\n",
       " 'eval_steps_per_second': 0.78,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2e9060c-2976-43e0-852e-b9fb4ab6398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_roberta_outputs = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93ff7fc5-968e-43f7-8104-0802082ef9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_roberta_predictions = finetuned_roberta_outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "487818c4-daa5-4148-b00f-bd641ab5e5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine-tuned roberta accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"fine-tuned roberta accuracy: \", round(accuracy_score(test_dataset[\"label\"], finetuned_roberta_predictions), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0efa276-2f44-4edf-95ec-18ace5a9d42b",
   "metadata": {},
   "source": [
    "### Load fine-tuned Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11a3a2df-abe7-4489-b900-b2c6b1928e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "finetuned_model = RobertaForSequenceClassification.from_pretrained(model_id, config=config).to(device)\n",
    "lora_weights = save_dir + \"/checkpoint-667\" # 667 training steps\n",
    "finetuned_model = PeftModel.from_pretrained(\n",
    "    finetuned_model, \n",
    "    lora_weights, \n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\", \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
